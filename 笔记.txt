机器学习：
    算法分类：
        监督学习
            类别--分类问题
            连续性的数据--回归问题
        无监督学习
            聚类算法
            anomaly detection异常检测
    机器学习开发流程
        获取数据
        数据处理
        特征工程
        算法训练--模型
        模型评估
        应用
    可用数据集
        sklearn（容易上手、文档完善、丰富api）、kaggle、uci
        训练数据、测试数据（20-30%）
    数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已
        将任意数据转换成机器理解的
            字典特征提取
                应用场景：
                    数据集当中特征比较多
                    本身数据是字典类型
            文本特征提取
                TF-IDF评估一个词或多个词的重要程度
                TF-词频  IDF-逆向文档频率  TF-IDF = TF * IDF
    特征预处理
        归一化
            对原始数据进行转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内
            鲁棒性（不确定性的扰动下，具有保持某种性能不变的能力）较差，只适合小数据场景
        标准化
            对原始数据进行变换把数据变换到均值为0，标准差为1的范围内
            异常点对标准化影响较小
    特征降维
        在某些限定条件下，降低随机变量（特征）个数，得到一组‘不相关’主变量的过程
        特征之间相关性较强，对算法学习预测影响较大
        方式：
            特征选择：数据中包含冗余或相关变量，旨在从原有特征中找出主要特征
            方法：
                过滤式：1方差选择法：低方差特征过滤   2相关系数：特征之间的相关程度 皮尔逊相关系数
                嵌入式：1决策树 2正则化 3深度学习
                包裹式：LVW
            主成分分析：高维数据转换成低维数据，可能会舍弃原有数据，创造新的变量
                作用：数据维数压缩，尽可能降低维数（复杂度），损失少量信息
                应用：回归分析或者聚类分析当中
        非线性降维KPCA计算开销较大
        流形学习是一类借鉴了拓扑流形概念的降维方法
    分类算法
        sklearn转换器和估计器
            转换器--特征工程父类
                1.实例化2.调用fit_transform
            估计器
                1.实例化一个estimator 2.estimator.fit()计算 3.模型评估
        KNN算法又称K-近邻算法--learn03 and learn04
            如果一个样本在特征空间中的k个最相似的样本中的类别，那该样本也属于这个类别
            距离公式：欧式距离、曼哈顿距离、明可夫斯基距离
            k=1容易受到异常点影响  但取得过大，会受到样本不均衡的影响
            优点：简单易于理解易于实现
            缺点：k值  懒惰算法 计算大内存消耗大
        模型选择与调优--learn04
            交叉验证，把训练集分成训练和验证  小数据集能体现其优势、但增加了计算量
            超参数搜索-网格搜索   可以最优选择knn算法里的K值
        朴素贝叶斯算法--learn05
            朴素：特征与特征之间相互独立
            应用场景：文本分类
            概率为0时采用拉普拉斯平滑系数
        决策树--learn06
            信息熵：度量样本集合纯度最常用的指标  决策树的分支结点所包含的样本尽可能属于同一类别，即结点的纯度(purity)越来越高
            信息增益越大则划分的纯度越大，信息增益最大的应该被选为划分属性
            基尼指数最小的作为最优划分属性
            过拟合时则需要剪枝处理
            优点 可视化
        随机森林--learn06
            是集成学习方法的一种
            特征随机
                从M个特征中随机抽m个特征   M>>m
            训练集随机
                bootStrap随机有放回抽样
            能有效处理高维特征的样本
        支持向量机
            能使用核函数   通过核技巧来构造核函数
    聚类算法
        LVQ算法（向量量化法）
        高斯混合聚类（概率）等等
    回归算法
        线性回归--learn07
            回归问题  连续性的数据
                应用场景：房价预测  销售额预测  贷款额度预测
            找到一种函数关系来表示特征值与目标值的关系
            自变量一次或参数一次都可以算是线性模型
            线性关系一定是线性模型  线性模型不一定是线性关系
            目标：求模型参数
            损失函数：减少损失则越靠近目标函数   最小二乘法/均方误差
                优化算法：正规方程（时间复杂度高）   梯度下降（用的更多）GD（梯度下降法）、SGD（随机梯度下降）、SAG（随机平均梯度下降法）（岭回归和逻辑回归有用到）
                梯度下降是用来求函数最小值的算法，这样就可以求出代价函数的最小值
            代价函数：整个训练集上所有样本误差的平均
            损失函数、代价函数都是用来评价模型的好坏的，越小说明模型越好
        回归性能评估
            均方误差
        欠拟合与过拟合
            特征太少或太多  模型过于简单或复杂
            解决：正则化  L1删除某个特征值的影响 L2(更常用)削弱某个特征值的影响   损失函数+惩罚项
        岭回归--线性回归改进  加了L2正则化--learn07
        逻辑回归（分类）--二分类--learn08
            应用：广告点击率、是否是垃圾邮件、是否患病、金融诈骗、虚假账号
            激活函数sigmoid  将线性回归的结果带入  映射到0-1
            损失函数(代价)--对数似然损失
          logistic回归函数又称sigmoid函数  将函数映射到0-1
        softmax回归--多分类问题
        交叉熵损失函数
        感知器（机）可谓是最简单的人工神经网络只有一个神经元  
        感知机：两层神经元组成
        分类的评估方法 
            精确率与召回率（二分类用的多）
                混淆矩阵
                精确率：预测结果为正例样本中真实为正例的比例
                召回率：真实为正例样本中预测结果为正例的比例
            F1-score反映模型的稳健性
            ROC曲线与AUC指标--衡量样本不均衡下的评估
                AUC指标在0.5-1之间  越高越好   只能评估二分类
        ‘基函数’
    模型保存与加载
        joblib.dump()保存
        joblib.load()加载
    无监督学习
        无监督学习-->无目标值
        K-means算法--聚类算法  --learn09
            性能评估--轮廓系数 系数越接近1越好  -1则越差
    补充知识：
        对模型的性能进行评估
            假设检验
            交叉验证t检验
            McNemar检验
            Friedman检验


深度学习部分：
    特征提取：传统的特征提取一般是和预测模型的学习分离的，先抽取有效的特征，再基于特征进行训练得出一个学习模型。
            而将特征提取与训练有机地统一到一个模型的这种表示方法叫深度学习。
    特征提取与表示学习  前者是基于任务或先验对去除无用特征，而后者通过深度模型学习高层语义特征
    核心问题：贡献度分配问题  从原始数据到结果的过程之间的重要性未知
        解决该问题-->偏导数

    PAC可能近似正确理论--帮助分析一个方法在什么条件下可以学到一个近似正确的分类器
    误差逆传播（简称BP，又称反向传播算法）--神经网络算法  BP网络一般指BP算法训练的多层前馈神经网络
    标准BP与累积BP算法的区别类似于随机梯度下降与标准梯度下降
    其他常见的神经网络：
        RBF网络
        ART网络
        SOM网络
        Elman网络
    多隐层神经网络难以直接用经典算法进行
        1.无监督逐层训练是多隐层网络训练的有效手段，预训练+微调
        2.权共享--卷积神经网络发挥重要作用
    向量机SVM

    集成学习两大类
        1.Boosting   AdaBoost--基学习器的线性组合     关注降低偏差
        2.Bagging and 随机森林

    激活函数的性质
        s型函数例如sigmoid函数、斜坡函数例如ReLU(x) = max(0,x)、复合函数例如Swish函数和高斯误差线性单元
        连续并可导的非线性函数，要尽可能简单，有利于提高网络计算效率
        值域要在一个合适的区间内，否则会影响训练的效率和稳定性
    自动微分
        计算梯度

    神经网络结构（常用的有三种）--分布式并行处理模型
        前馈网络
            可以看作一个函数，通过简单非线性函数的多次复合，朝一个方向传播
            全连接网络 
            卷积神经网络也是前馈网络
                特点
                    局部连接
                    权重共享
                    空间或时间上的次采样
                输出数为input-kernel+1 
                滑动步长、零填充
                目前默认卷积为等宽卷积即input数等于输出数
                汇聚层
                    减少了连接的个数，但没有减少每个特征映射的神经元个数，采用汇聚层来减少（最大汇聚、平均汇聚）
                空洞卷积增加输出单元的感受野
                低维特征转成高维特征-->转置卷积/微步卷积
                残差网络通过增加‘直连边’来提高信息的转播效率
                应用
                    alphaGo围棋
                    目标检测-RCN
                    图像分割
                    OCR文字识别
        记忆网络
            不但可以接收其他神经元的信息还可以接收自己的历史信息
            循环神经网络RNN
                简单循环网络SRN
                长程依赖问题--梯度爆炸或梯度消失
                    爆炸：权重衰减、梯度截断
                    消失：改进模型（难）
                        循环边改为线性依赖关系--模型效果变差
                        增加非线性
                        门控机制--控制信息的累积速度，有选择地加入新信息，并有选择地遗忘之前的信息
                            基于门控的循环神经网络
                                门控循环单元GRU
                                长短期记忆网络LSTM
                应用
                    语言模型--自然语言理解->一个句子的可能性/合理性
                    机器翻译
                    看图说话/说话转图
                    ......
            时延神经网络
                建立一个额外的延时单元，用来存储网络的历史信息（如输入、输出、隐状态等）
            自回归模型、非线性自回归模型--有外部输入
        图网络
            处理图结构
    网络优化
        难点
            结构差异大
                没有通用的优化算法
                超参数多
            非凸优化问题
                参数初始化
                逃离局部最优或鞍点（高维）
                    鞍点：梯度为0，在有些维度上局部最小，有些是局部最大
                平坦最小值
                    在最小值附件的领域内所有点对应的训练损失都比较接近
                    大部分的局部最小解是等价的
            梯度消失或爆炸问题
        改善方法
            更有效的优化算法来提高优化方法的效率和稳定性
                动态学习率调整7.3节
                梯度估计修正7.4节
            更好的参数初始化方法、数据预处理方法来提高优化效率7.5节---后面
                参数初始化不能全为0，会导致对称权重问题
                批量规范化无法应用到循环神经网络
                循环神经网络用层规范化更多
            修改网络结构来得到更好的优化地形
                好的优化地形通常比较平滑
                使用ReLU激活函数、残差连接、逐层归一化等
            使用更好的超参数优化方法



