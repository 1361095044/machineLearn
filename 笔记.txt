机器学习：
    算法分类：
        监督学习
            类别--分类问题
            连续性的数据--回归问题
        无监督学习
            聚类算法
            anomaly detection异常检测
    机器学习开发流程
        获取数据
        数据处理
        特征工程
        算法训练--模型
        模型评估
        应用
    可用数据集
        sklearn（容易上手、文档完善、丰富api）、kaggle、uci
        训练数据、测试数据（20-30%）
    数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已
        将任意数据转换成机器理解的
            字典特征提取
                应用场景：
                    数据集当中特征比较多
                    本身数据是字典类型
            文本特征提取
                TF-IDF评估一个词或多个词的重要程度
                TF-词频  IDF-逆向文档频率  TF-IDF = TF * IDF
    特征预处理
        归一化
            对原始数据进行转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内
            鲁棒性（不确定性的扰动下，具有保持某种性能不变的能力）较差，只适合小数据场景
        标准化
            对原始数据进行变换把数据变换到均值为0，标准差为1的范围内
            异常点对标准化影响较小
    特征降维
        在某些限定条件下，降低随机变量（特征）个数，得到一组‘不相关’主变量的过程
        特征之间相关性较强，对算法学习预测影响较大
        方式：
            特征选择：数据中包含冗余或相关变量，旨在从原有特征中找出主要特征
            方法：
                过滤式：1方差选择法：低方差特征过滤   2相关系数：特征之间的相关程度 皮尔逊相关系数
                嵌入式：1决策树 2正则化 3深度学习
            主成分分析：高维数据转换成低维数据，可能会舍弃原有数据，创造新的变量
                作用：数据维数压缩，尽可能降低维数（复杂度），损失少量信息
                应用：回归分析或者聚类分析当中
    分类算法
        sklearn转换器和估计器
            转换器--特征工程父类
                1.实例化2.调用fit_transform
            估计器
                1.实例化一个estimator 2.estimator.fit()计算 3.模型评估
        KNN算法又称K-近邻算法--learn03 and learn04
            如果一个样本在特征空间中的k个最相似的样本中的类别，那该样本也属于这个类别
            距离公式：欧式距离、曼哈顿距离、明可夫斯基距离
            k=1容易受到异常点影响  但取得过大，会受到样本不均衡的影响
            优点：简单易于理解易于实现
            缺点：k值  懒惰算法 计算大内存消耗大
        模型选择与调优--learn04
            交叉验证，把训练集分成训练和验证  小数据集能体现其优势、但增加了计算量
            超参数搜索-网格搜索   可以最优选择knn算法里的K值
        朴素贝叶斯算法--learn05
            朴素：特征与特征之间相互独立
            应用场景：文本分类
            概率为0时采用拉普拉斯平滑系数
        决策树--learn06
            信息熵：度量样本集合纯度最常用的指标  决策树的分支结点所包含的样本尽可能属于同一类别，即结点的纯度(purity)越来越高
            信息增益越大则划分的纯度越大，信息增益最大的应该被选为划分属性
            基尼指数最小的作为最优划分属性
            过拟合时则需要剪枝处理
            优点 可视化
        随机森林--learn06
            是集成学习方法的一种
            特征随机
                从M个特征中随机抽m个特征   M>>m
            训练集随机
                bootStrap随机有放回抽样
            能有效处理高维特征的样本
    回归算法
        线性回归--learn07
            回归问题  连续性的数据
                应用场景：房价预测  销售额预测  贷款额度预测
            找到一种函数关系来表示特征值与目标值的关系
            自变量一次或参数一次都可以算是线性模型
            线性关系一定是线性模型  线性模型不一定是线性关系
            目标：求模型参数
            损失函数：减少损失则越靠近目标函数   最小二乘法/均方误差
                优化算法：正规方程（时间复杂度高）   梯度下降（用的更多）GD（梯度下降法）、SGD（随机梯度下降）、SAG（随机平均梯度下降法）（岭回归和逻辑回归有用到）
                梯度下降是用来求函数最小值的算法，这样就可以求出代价函数的最小值
            代价函数：整个训练集上所有样本误差的平均
            损失函数、代价函数都是用来评价模型的好坏的，越小说明模型越好
        回归性能评估
            均方误差
        欠拟合与过拟合
            特征太少或太多  模型过于简单或复杂
            解决：正则化  L1删除某个特征值的影响 L2(更常用)削弱某个特征值的影响   损失函数+惩罚项
        岭回归--线性回归改进  加了L2正则化--learn07
        逻辑回归（分类）--二分类--learn08
            应用：广告点击率、是否是垃圾邮件、是否患病、金融诈骗、虚假账号
            激活函数sigmoid  将线性回归的结果带入  映射到0-1
            损失函数(代价)--对数似然损失
        分类的评估方法
            精确率与召回率（二分类用的多）
                混淆矩阵
                精确率：预测结果为正例样本中真实为正例的比例
                召回率：真实为正例样本中预测结果为正例的比例
            F1-score反映模型的稳健性
            ROC曲线与AUC指标--衡量样本不均衡下的评估
                AUC指标在0.5-1之间  越高越好   只能评估二分类
    模型保存与加载
        joblib.dump()保存
        joblib.load()加载
    无监督学习
        无监督学习-->无目标值
        K-means算法--聚类算法  --learn09
            性能评估--轮廓系数 系数越接近1越好  -1则越差
    补充知识：
        对模型的性能进行评估
            假设检验
            交叉验证t检验
            McNemar检验
            Friedman检验

深度学习部分：
    神经网络
    感知机：两层神经元组成
    误差逆传播（简称BP，又称反向传播算法）--神经网络算法  BP网络一般指BP算法训练的多层前馈神经网络
    标准BP与累积BP算法的区别类似于随机梯度下降与标准梯度下降
    其他常见的神经网络：
        RBF网络
        ART网络
        SOM网络
        Elman网络
    随着云计算大数据时代的到来，深度学习开始
    多隐层神经网络难以直接用经典算法进行
        1.无监督逐层训练是多隐层网络训练的有效手段，预训练+微调
        2.权共享--卷积神经网络发挥重要作用
    向量机SVM

    集成学习两大类
        1.Boosting   AdaBoost--基学习器的线性组合     关注降低偏差
        2.Bagging and 随机森林

